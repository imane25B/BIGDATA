# ğŸš€ TP Big Data â€“ Pipeline de Traitement Temps RÃ©el

## ğŸ‘©â€ğŸ’» RÃ©alisÃ© par :  
**BACHIRI Imane**

---

## ğŸ“Œ Contexte

Dans un monde oÃ¹ les **donnÃ©es massives** sont omniprÃ©sentes, leur traitement en **temps rÃ©el** est devenu indispensable. Ce TP met en Å“uvre un pipeline de donnÃ©es utilisant les outils majeurs du Big Data :

- **Apache Kafka** pour la gestion des flux de donnÃ©es.
- **Apache Spark** pour le traitement en temps rÃ©el.
- **Docker** pour l'orchestration de l'environnement.
- **MinIO** pour le stockage objet (format Parquet).

---

## ğŸ¯ Objectifs pÃ©dagogiques

- ğŸ“¦ DÃ©ployer un environnement Big Data conteneurisÃ©.
- ğŸ”„ GÃ©rer un flux de donnÃ©es via Kafka.
- âš™ï¸ Traiter et enrichir les donnÃ©es avec Spark Streaming.
- ğŸª£ Stocker les rÃ©sultats dans MinIO au format **Parquet**.

---

## ğŸ“š CompÃ©tences dÃ©veloppÃ©es

âœ… DÃ©ploiement dâ€™un Ã©cosystÃ¨me Big Data via Docker  
âœ… Configuration de Kafka pour le streaming  
âœ… IntÃ©gration de Spark Streaming avec Kafka  
âœ… Application de transformations sur flux de donnÃ©es  
âœ… Ã‰criture/lecture de fichiers Parquet via MinIO

---

## ğŸ”§ PrÃ©-requis techniques

- Docker Desktop installÃ©
- Python 3
- Java 8+
- Apache Spark
- Kafka (via Docker)
- MinIO (via Docker)

---

## ğŸ—ï¸ Architecture du projet

```

+-------------+      Kafka      +-------------------+      Parquet      +---------+
\| capteur.py  |  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ | consumer\_spark.py |  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ | MinIO   |
\| (Producer)  |                |   (Spark Streaming)|                  | Storage |
+-------------+                +-------------------+                  +---------+

````

---

## âš™ï¸ Composants testÃ©s

| Composant | VÃ©rification |
|----------|--------------|
| Docker   | âœ… `docker ps` |
| Java     | âœ… `java -version` |
| Spark    | âœ… `spark-submit --version` |

---

## ğŸš€ Mise en Å“uvre

### 1ï¸âƒ£ DÃ©marrage des services Docker

```bash
docker-compose up -d
````

VÃ©rifier que les services Kafka, Zookeeper, MinIO, Spark sont bien lancÃ©s.

---

### 2ï¸âƒ£ Producteur Kafka (Python)

Fichier : `capteur.py`

* GÃ©nÃ¨re des donnÃ©es simulÃ©es.
* Envoie les messages JSON dans un **topic Kafka**.

Exemple de message envoyÃ© :

```json
{"transaction_id": 123, "montant": 100, "devise": "USD", "timestamp": "..."}
```

---

### 3ï¸âƒ£ Traitement Spark Streaming

Fichier : `consumer_spark.py`

* Lit les messages depuis Kafka.
* Convertit les devises (USD â†’ EUR).
* Transforme la date, ajoute le fuseau horaire.
* Filtre les erreurs / donnÃ©es incomplÃ¨tes.
* Enregistre les rÃ©sultats dans **MinIO** (`/output/`) en **Parquet**.

---

### 4ï¸âƒ£ AccÃ¨s aux rÃ©sultats sur MinIO

* URL : [http://localhost:9001](http://localhost:9001)
* **Username** : `Minio`
* **Password** : `Minio123`

VÃ©rifie les buckets :

* `/transactions/output/` â†’ fichiers Parquet
* `/transactions/checkpoint/` â†’ Ã©tat Spark

---

## ğŸ§ª ExÃ©cution des scripts

1. Lancer les containers :

```bash
docker-compose up -d
```

2. Lancer Spark (consumer) :

```bash
spark-submit consumer_spark.py
```

3. Lancer le producteur (Python) :

```bash
python capteur.py
```

---

## âœ… RÃ©sultat attendu

* Kafka reÃ§oit les messages produits par `capteur.py`
* Spark traite les messages et Ã©crit les donnÃ©es enrichies dans MinIO
* Les fichiers `.parquet` sont accessibles et lisibles par Spark ou tout autre moteur Big Data

---

## ğŸ’¡ AmÃ©liorations possibles

* Ajout de monitoring avec Grafana + Prometheus
* Stockage vers un Data Lake (HDFS ou S3 simulÃ©)
* Ajout dâ€™un module de machine learning pour scoring des transactions

---

## ğŸ“œ Licence

Projet acadÃ©mique â€“ Usage libre Ã  des fins pÃ©dagogiques.
